\documentclass[10pt,english]{article}

\usepackage[utf8]{inputenc}
\usepackage[margin=1in,letterpaper]{geometry}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amssymb}

\usepackage{graphicx}
\usepackage[parfill]{parskip}
%\usepackage{enumitem}

\title{Optimistic POMDP Notes}
\author{Daniel (Zhaohan) Guo}
\date{\today}

\begin{document}

\maketitle

\section{Hoeffdings Confidence Intervals for bounded random variable}

\begin{align}
\Pr(|X-\mathbb{E}(X)| > \epsilon) &< \alpha \\
\Pr(|X-\mathbb{E}(X)| > \epsilon) &< 2e^{-2\epsilon^2n}\\
\alpha &= 2e^{-2\epsilon^2n}\\
\implies \epsilon = \sqrt{\frac{1}{2n}\log\left(\frac{2}{\alpha}\right)}
\end{align}
so $\epsilon$ is the radius of the confidence interval.

\section{EM with rewards}

Let the transition matrix be $T(s'|s,a) = p(s'|s,a)$. Let the observation matrix be $Z(z|s',a)=p(z|s',a)$. Let the expected reward be $R(s,a)$, and let the reward matrix (when using rewards as observations) be $R(r|s,a)=p(r|s,a)$. Let $T_{\lambda},Z_\lambda,R_\lambda$ be the matrices according to the model $\lambda$.

If we just want to estimate rewards (without treating them as observations) then we can do it very simply

\begin{align}
\tilde{R}(s,a) &= \frac{\sum_{t=1}^{T}\gamma_{s}(t)\delta(a_t,a) r_t}{\sum_{t=1}^{T}\gamma_{s}(t)\delta(a_t,a)}
\end{align}

If we were to treat the rewards as a set of discrete observations, then the update for each reward $k$ is

\begin{align}
\tilde{R}(r|s,a) &= \frac{\sum_{t=1}^{T}\delta(r_t,r) \gamma_{s}(t)\delta(a_t,a)}{\sum_{t=1}^{T}\gamma_{s}(t)\delta(a_t,a)}
\end{align}

and if we took expected value of the reward that would be

\begin{align}
\tilde{R}(s,a) &= \sum_r r\tilde{R}(r|s,a)
\end{align}

which reduces to the first equation.

Now let's derive the other quantities when treating rewards as observations is a discrete distribution. Note in the history, there is no first observation. The actions are given.
\begin{align}
\alpha_s(t) &= p(r_{1..t},z_{2..t},s_t|a_{1..T},\lambda)
\end{align}
and now for how to calculate it recursively
\begin{align}
\alpha_s(1) &= p(r_1,s_1=s|a_1,\lambda) \\
&= p(r_1|s_1=s,a_1,\lambda)p(s_1=s|a_1,\lambda) \\
&= R_\lambda(r_1|s,a_1)\pi_s \\
\alpha_s(t+1) &= p(r_{1..t+1},z_{2..t+1},s_{t+1}=s|a_{1..T},\lambda) \\
&= \sum_{s'} p(r_{1..t+1},z_{2..t+1},s_{t+1}=s,s_t=s'|a_{1..T},\lambda) \\
&= \sum_{s'} p(r_{t+1},z_{t+1},s_{t+1}=s|s_t=s',r_{1..t},z_{2..t},a_{1..T},\lambda) p(s_t=s',r_{1..t},z_{2..t}|a_{1..T},\lambda) \\
&= \sum_{s'} p(r_{t+1},z_{t+1},s_{t+1}=s|s_t=s',r_{1..t},z_{2..t},a_{1..T},\lambda) \alpha_{s'}(t) \\
&= \sum_{s'} p(r_{t+1},z_{t+1}|s_{t+1}=s,a_{1..T},\lambda) p(s_{t+1}=s|s_t=s',a_{1..T},\lambda) \alpha_{s'}(t) \\
&= \sum_{s'} p(r_{t+1},z_{t+1}|s_{t+1}=s,a_{t},a_{t+1},\lambda) T_\lambda(s|s',a_t) \alpha_{s'}(t) \label{eq:alpha_joint} \\
&= \sum_{s'} R_\lambda(r_{t+1}|s,a_{t+1}) Z_\lambda(z_{t+1}|s,a_t) T_\lambda(s|s',a_t) \alpha_{s'}(t) \\
&= R_\lambda(r_{t+1}|s,a_{t+1}) Z_\lambda(z_{t+1}|s,a_t) \sum_{s'} T_\lambda(s|s',a_t) \alpha_{s'}(t)
\end{align}
In general, the rewards obs and regular obs could be dependent, which means you need to specify the joint distribution as in equation \ref{eq:alpha_joint}. Just for completeness, the equations for when not using reward as obs is the same except with the reward matrix taken out:
\begin{align}
\alpha_s(1) &= \pi_s \\
\alpha_s(t+1) &= Z_\lambda(z_{t+1}|s,a_t) \sum_{s'} T_\lambda(s|s',a_t) \alpha_{s'}(t)
\end{align}
Also note that we can loosen the definition slightly and get something that's a constant multiple off
\begin{align}
A_s(1) &= C_t \alpha_s(1) \\
A_s(t+t) &= C_{t+1} R_\lambda(r_{t+1}|s,a_{t+1}) Z_\lambda(z_{t+1}|s,a_t) \sum_{s'} T_\lambda(s|s',a_t) A_{s'}(t) \\
&= C_{t+1} C_t R_\lambda(r_{t+1}|s,a_{t+1}) Z_\lambda(z_{t+1}|s,a_t) \sum_{s'} T_\lambda(s|s',a_t) \alpha_{s'}(t) \\
&= C_{t+1} \alpha_s(t+1)
\end{align}
by absorbing constants (also works with just the obs and no reward obs). Using this looser formulation, we can normalize the numbers at every $t$ thus avoiding floating point overflow or underflow errors.

We can use this to calculate the likelihood:
\begin{align}
p(r_{1..T},z_{2..T}|a_{1..T},\lambda) &= \sum_{s} \alpha_s(T)
\end{align}

Next is the backward procedure which is similar.
\begin{align}
\beta_s(t) &= p(r_{t+1..T},z_{t+1..T}|s_t=s,a_{1..T},\lambda)
\end{align}
and now for how to calculate it recursively
\begin{align}
\beta_s(T) &= 1\\ 
\beta_s(t) &= \sum_{s'} p(r_{t+1..T},z_{t+1..T},s_{t+1}=s'|s_t=s,a_{1..T},\lambda) \\
\beta_s(t) &= \sum_{s'} p(r_{t+1},z_{t+1}|s_{t+1}=s',a_{1..T},\lambda) p(s_{t+1}=s',r_{t+2..T},z_{t+2..T},|s_t=s,a_{1..T},\lambda) \\
\beta_s(t) &= \sum_{s'} R_\lambda(r_{t+1}|s',a_{t+1}) Z_\lambda(z_{t+1}|s',a_t) p(s_{t+1}=s',r_{t+2..T},z_{t+2..T}|s_t=s,a_{1..T},\lambda) \\
\beta_s(t) &= \sum_{s'} R_\lambda(r_{t+1}|s',a_{t+1}) Z_\lambda(z_{t+1}|s',a_t) p(r_{t+2..T},z_{t+2..T}|s_{t+1}=s',a_{1..T},\lambda) p(s_{t+1}=s'|s_t=s,a_{t},\lambda) \\
\beta_s(t) &= \sum_{s'} R_\lambda(r_{t+1}|s',a_{t+1}) Z_\lambda(z_{t+1}|s',a_t) \beta_{s'}(t+1) T_\lambda(s'|s,a_t)
\end{align}
if we don't use the rewards as obs, then just remove that term from the equation. Also we can loosen this a bit and define $B_s(t) = C_t \beta_s(t)$ similarly as before. We can renormalize at every $t$ to avoid under/overflow. Next up is putting these two together
\begin{align}
\gamma_s(t) &= p(s_t=s|r_{1..T},z_{2..T},a_{1..T},\lambda) \\
&= \frac{p(r_{1..T},z_{2..T},s_t=s|a_{1..T},\lambda)}{\sum_{s'} p(r_{1..T},z_{2..T},s_t=s'|a_{1..T},\lambda)} \\
&= \frac{\alpha_s(t)\beta_s(t)}{\sum_{s'} \alpha_{s'}(t)\beta_{s'}(t)} \\
&= \frac{A_s(t)B_s(t)}{\sum_{s'} A_{s'}(t)B_{s'}(t)}
\end{align}
So far reward obs fit right in very easily into the equations as you'd expect obs to show up. Next up is estimating transitions using expected transitions. \(\xi_{i,j,a}\) is the probability that you transition from state $i$ to $j$ when doing action $a$ in state $i$.
\begin{align}
\xi_{i,j,a}(t) &= \delta(a_t=a)p(s_t=i,s_{t+1}=j|r_{1..T},z_{2..T},a_{1..T},\lambda) \\
\xi_{i,j,a}(t) &= \frac{\delta(a_t=a)p(r_{1..T},z_{2..T},s_t=i,s_{t+1}=j|a_{1..T},\lambda)}{\sum_{i,j} p(r_{1..T},z_{2..T},s_t=i,s_{t+1}=j|a_{1..T},\lambda)}\\
p(r_{1..T},z_{2..T},s_t=i,s_{t+1}=j|a_{1..T},\lambda) &= p(r_{1..t},z_{2..t},r_{t+1..T},z_{t+1..T},s_t=i,s_{t+1}=j|a_{1..T},\lambda) \\
&= p(r_{t+1..T},z_{t+1..T},s_{t+1}=j|s_t=i,a_{1..T},\lambda) \alpha_i(t) \\
&= p(r_{t+2..T},z_{t+2..T},r_{t+1},z_{t+1}|s_{t+1}=j,a_{1..T},\lambda) T_\lambda(j|i,a_t) \alpha_i(t) \\
&= p(r_{t+1},z_{t+1}|s_{t+1}=j,a_{t},a_{t+1},\lambda) T_\lambda(j|i,a_t) \alpha_i(t) \beta_j(t+1) \\
&= R_\lambda(r_{t+1}|j,a_{t+1}) Z_\lambda(z_{t+1}|j,a_t) T_\lambda(j|i,a_t) \alpha_i(t) \beta_j(t+1)
\end{align}
Where the reward matrix is removable if you don't use the reward as obs. Also note we can substitute in $A_s(t)$ and $B_s(t)$ because the constants will cancel out. And then to actually do the prediction of course is just
\begin{align}
\tilde{T}(j|i,a) &= \frac{\sum_{t=1}^{T-1} \xi_{i,j,a}(t)}{\sum_{t=1}^{T-1} \gamma_i(t)\delta(a_t,a)}
\end{align}
which is up to $T-1$ since $\xi_{i,j,a}(t)$ is only defined up to $T-1$. And then to estimate the reward obs, you calculate the expected number of times a reward $r$ is observed from state $s$ executing action $a$
\begin{align}
\tilde{R}(r|s,a) &= \frac{\sum_{t=1}^T \gamma_s(t)\delta(a_t,a)\delta(r_t,r)}{\sum_{t=1}^T \gamma_s(t)\delta(a_t,a)}
\end{align}
and just for completeness the update for regular obs is very similar (except you need to check the previous action)
\begin{align}
\tilde{Z}(z|s',a) &= \frac{\sum_{t=1}^T \gamma_{s'}(t)\delta(a_{t-1},a)\delta(z_t,z)}{\sum_{t=1}^T \gamma_{s'}(t)\delta(a_{t-1},a)}
\end{align}

\section{POMDP with reward obs}

Let's define the history of a POMDP with discrete rewards obs and use the fact that there's no first obs to make things cleaner
\begin{align}
h_t &= a_{1..t},o_{2..t},r_{1..t} \\
h_1 &= a_1,r_1 \\
h_2 &= a_1,a_2,o_2,r_1,r_2
\end{align}
Now let's look at belief update with these where there's some $b_0$
\begin{align}
b_s(t+1) &= p(s_{t+1}=s|h_{t+1}) \\
&= p(s_{t+1}=s|o_{t+1},r_{t+1},a_{t+1},h_{t}) \\
&= \frac{p(o_{t+1},r_{t+1},s_{t+1}=s|a_{t+1},h_{t})}{p(o_{t+1},r_{t+1}|a_{t+1},h_{t})} \\
&\propto {p(o_{t+1},r_{t+1},s_{t+1}=s|a_{t+1},h_{t})} \\
&= \sum_{s'} p(o_{t+1},r_{t+1},s_{t+1}=s,s_t=s'|a_{t+1},h_{t}) \\
&= \sum_{s'} p(o_{t+1},r_{t+1}|s_{t+1}=s,a_t,a_{t+1}) p(s_{t+1}=s|s_t=s',a_{t}) p(s_t=s'|h_{t})\\
&= \sum_{s'} p(o_{t+1},r_{t+1}|s_{t+1}=s,a_t,a_{t+1}) t(s|s',a_t) b_{s'}(t) \\
b_s(1) &= p(s_{1}=s|r_{1},a_{1}) \\
&\propto p(s_{1}=s,r_{1}|a_{1}) \\
&= p(r_{1}|s_{1}=s,a_{1})b_s(0)
\end{align}

Now think about what a policy is. A one step policy is just a single action. A two step policy is to take a single action, see a reward and observation, then take another action. Then how to do a one step policy evaluation?
\begin{align}
\alpha_{\pi}(s) &= r(s,\pi_0) + \gamma\sum_{r}\sum_{s'}\sum_{o'}p(s',r,o'|s,\pi_0)\alpha_{\pi(r,o')}(s') \\
&= r(s,\pi_0) + \gamma\sum_{r}\sum_{s'}\sum_{o'}\alpha_{\pi(r,o')}(s')p(r|s,\pi_0)p(o'|s',\pi_0)p(s'|s,\pi_0) \\
&= r(s,\pi_0) + \gamma\sum_{r}p(r|s,\pi_0)\sum_{s'}p(s'|s,\pi_0)\sum_{o'}p(o'|s',\pi_0)\alpha_{\pi(r,o')}(s')
\end{align}
so to do a belief point backup, we need to pick an alpha vector for each possible reward and observation pair, and then compute this update. Then pick the alpha vector that best maximizes the dot product between it and the belief point. The optimistic version is:
\begin{align}
\alpha_{\pi}(s) &= r(s,\pi_0) + \gamma \max_{P_R} \sum_{r}p(r|s,\pi_0)\max_{P_T} \sum_{s'}p(s'|s,\pi_0) \max_{P_O} \sum_{o'}p(o'|s',\pi_0) \alpha_{\pi(r,o')}(s')
\end{align}
so to calculate these maximums, we can just do each max separate. The inner most maximum we have to calculate for every $(o',s')$. The middle maximum we do for every $s'$, and so on.

\section{Old Main POMDP Theorem}

\subsubsection{Generalized Induced Inequality}
Suppose $M$ and $M_{OPT}$ are exactly the same on some set of known actions (so all states with that action is known). Then let's consider all trajectories over the next $T$ steps. Those trajectories can be split up into two groups for each POMDP, ones where you escape and perform an unknown action, and ones where you don't and only perform known actions. For every trajectory where you don't escape, there is exactly the same trajectory for both POMDPs, because the belief state tracking is exactly the same, and since the policies are both the same (if we look at policies as mapping a belief to an action) then they perform the same actions, and so they have identical trajectories. So there is this one-to-one mapping, that means the difference between trajectories of those kind is altogether $0$. It also means that the probability of escaping is exactly the same, since they have the exact same set of trajectories that don't escape (and everything adds up to $1$). Then it means the difference in the value functions is exactly determined by the trajectories that escape, which can be bounded.
\begin{align}
V^{\pi}_M(b,T) - V^{\pi}_{M_{OPT}}(b,T) &= [V^A_M(b,T|\neg W)(1-w) - V^{\pi}_{M_{OPT}}(b,T|\neg W)(1-w)] \\
& + [V^A_M(b,T|W)w - V^{\pi}_{M_{OPT}}(b,T|W)w]\\
&\geq -V_{max}w
\end{align}

\subsection{Old Main Theorem with just some set of known state--action pairs}

Let $A$ be our algorithm's policy at time $t$. Assume $A$ doesn't change for the next $T$ steps. Let $M$ be the true POMDP. Let $M_{OPT}$ be the optimistic POMDP that our algorithm believes in. Let $\pi$ be the optimal policy for $M_{OPT}$. Let there be a set of known states, where the confidence intervals around all the state--action pairs of those states are very small. Let $W$ be the event that running $A$ in $M$ results in escaping to an unknown state in the next $T$ steps. Let $W'$ be the event that running $\pi$ in $M_{OPT}$ results in escaping to an unknown state in the next $T$ steps. Let $w=P(W)$ and similarly $w'=P(W')$. Then
\begin{align}
V^A_M(b,T) - V^{\pi}_{M_{OPT}}(b,T) &= [V^A_M(b,T|\neg W)(1-w) - V^{\pi}_{M_{OPT}}(b,T|\neg W')(1-w')] \\
& + [V^A_M(b,T|W)w - V^{\pi}_{M_{OPT}}(b,T|W')w']
\end{align}
let's suppose that $|w-w'|<d_w$ and also that $\max(w,w')<\epsilon_w$. Then the smallest possible value for that second term is when the first term is as small as possible and then second is as big as possible.
\begin{align}
[V^A_M(b,T|W)w - V^{\pi}_{M_{OPT}}(b,T|W')w'] &\geq 0 - V_{max}\epsilon_w = -V_{max}\epsilon_w
\end{align}
now as for the first difference, we can add in some extra terms to make the difference clearer
\begin{align}
 & [V^A_M(b,T|\neg W)(1-w) - V^{\pi}_{M_{OPT}}(b,T|\neg W')(1-w')] \\
 &= V^A_M(b,T|\neg W)(1-w) - V^A_M(b,T|\neg W)(1-w') + V^A_M(b,T|\neg W)(1-w') - V^{\pi}_{M_{OPT}}(b,T|\neg W')(1-w') \\
 &= V^A_M(b,T|\neg W)(w'-w) + (1-w')[V^A_M(b,T|\neg W) - V^{\pi}_{M_{OPT}}(b,T|\neg W')] \\
&\geq -V_{max}d_w + (1-w')[V^A_M(b,T|\neg W) - V^{\pi}_{M_{OPT}}(b,T|\neg W')] \\
&\geq -V_{max}d_w - V_{max}\epsilon_w + [V^A_M(b,T|\neg W) - V^{\pi}_{M_{OPT}}(b,T|\neg W')]
\end{align}
so now we're left with
\begin{align}
V^A_M(b,T) - V^{\pi}_{M_{OPT}}(b,T) &\geq -V_{max}d_w -2V_{max}\epsilon_w + [V^A_M(b,T|\neg W) - V^{\pi}_{M_{OPT}}(b,T|\neg W')]
\end{align}
now let's take a closer look at that difference conditioned on not escaping. If neither of them escape, then they are just acting in the known POMDP. They both use the exact same transition, observation, reward parameters, so their updates are going to be very similar. The only difference is that they are running in slightly different POMDPs.

\section{Example of Approximate Belief Tracking}

\subsection{Approximate Observations}

Suppose we have a 3 state POMDP $s_0, s_1, s_2$ where $s_0$ is the starting state. Let the policy be fixed, so that we can talk about the single transition and observation matrices. Suppose we know the transition probabilities with no error. $s_0$ transitions to $s_1$ and $s_2$ each with uniform probability. $s_1$ transitions to itself deterministically, and $s_2$ transitions to itself deterministically. There are three observations. Starting in $s_0$ gives observation $z_0$. $s_1$ gives observation $z_1$ with probability $p_1$ and $z_2$ with prob $1-p_1$, and $s_2$ gives $z_1$ with probability $p_2$ and $z_2$ with prob $1-p_2$. So the idea is that the system transitions randomly to either $s_1$ or $s_2$ and stays in that state, getting observations that give it a little bit of information about which state it is actually in, governed by the probs $p_1,p_2$.

Suppose $p_1 = 0.01$ and $p_2 = 0.02$. Suppose that we get a sequence of observations of just $z_1$. Then the belief will be updated to reflect that the more likely state is $s_2$. And this belief will continue to add more weight to $p_2$. However we may estimate those values to be off and instead think that $\hat{p}_1 = 0.02$ and $\hat{p}_2 = 0.01$. Now let's calculate just how much error there is for these two situations.

The initial belief is of course $\langle 0.5, 0.5 \rangle$. The transition matrix is just the identity for these two states, so after applying it it is still $\langle 0.5, 0.5 \rangle$. However now we come to the observations. And they end up scaling it by $\langle \frac{p_1 b_1}{p_1 b_1 + p_2 b_2} , \frac{p_2 b_2}{p_1 b_1 + p_2 b_2} \rangle$. Suppose WLG that $b_1 \leq b_2$ and $p_1 \leq p_2$, then $p_1 b_1 \leq p_2 b_2$, and
\begin{align}
\frac{p_2 b_2}{p_1 b_1 + p_2 b_2} / \frac{p_1 b_1}{p_1 b_1 + p_2 b_2} &= \frac{p_2 b_2}{p_1 b_1} \\
&= \left( \frac{p_2}{p_1} \right) \left( \frac{b_2}{b_1} \right)
\end{align}
so the ratio of the beliefs grows with a constant factor each iteration.

So if the estimates are off, then that means the constant factor is off.

\subsection{Quantifying additive and multiplicative differences}

Suppose $b_1 + b_2 = 1$ are non-negative reals and you know that $\frac{b_2}{b_1} = c$. Then what's $|b_2 - b_1|$? Well we can just solve the two equations
\begin{align}
& & b_1 + cb_1 &= 1 \\
\implies & & b_1 &= \frac{1}{1+c} \\
\implies & & b_2 &= \frac{c}{1+c} \\
\implies & & |b_2 - b_1 |  &= \frac{|1-c|}{1+c}
\end{align}

Note that $c$ and $\frac{1}{c}$ are symmetric cases.

Now suppose we have another belief vector with a different constant ratio
\begin{align}
|b_1 - b_3| &= \left| \frac{1}{1+c} - \frac{1}{1+d}\right| \\
&= \frac{|d-c|}{(1+c)(1+d)} \\
&= \frac{\delta}{(1+c)(1+c+\delta)} \text{ assume $c \leq d$}\\
&= \frac{1}{1+c} - \frac{1}{1+c+\delta}\\
\text{So } \frac{1}{1+c} - \frac{1}{1+c+\delta} &\leq \epsilon\\
\iff \delta &\leq \begin{cases}
\frac{1}{\frac{1}{1+c}-\epsilon} - 1 - c & \epsilon < \frac{1}{1+c}\\
\infty & \epsilon \geq \frac{1}{1+c}
\end{cases}\\
\text{So we want }  \frac{\delta}{(1+c)(1+c+\delta)} &\leq \epsilon\\
\text{So a sufficient condition is } \frac{\delta}{(1+c)^2} &\leq \epsilon\\
\Longleftarrow \delta &\leq (1+c)^2 \epsilon \\
\Longleftarrow \delta &\leq \epsilon\\
\text{What is a necessary condition?}
\end{align}

so we just need a small additive difference in the ratios. However that's the whole problem, since the ratios diverge multiplicatively.

\section{Homing}

It seems that we don't need to run in episodes over and over again like the original homing algorithm if we set in advance how close we want to be. First an outline of the algorithm
\begin{itemize}
\item first exploration, then exploitation
\item First, exploration, and with MoM that means after homing H, we take one (maybe a couple more?) samples, then run H, and collect some samples again, and over and over. So for every sample we need H many steps. Since the confidence intervals would be essentially over the actions, and we can pick what actions to take, we can uniformly sample all the actions enough to a desired accuracy for the confidence intervals. This will require exponentially many samples.
\item then we alternate homing and running the policy for some t steps, which will average close to $\epsilon$ reward
\end{itemize}

Alternatively, instead of this two-phase algorithm, we have many episodes of these two phases, in which we increase t every episode. So in the exploration part, we would only get enough new samples such that we can increase t by 1, and then repeat.

So confidence intervals around actions are $\frac{C}{\sqrt{n}}$ where $n$ is the number of times that action has been taken. The error in a $t$ length exploitation run is exponential in $t$ e.g. $\epsilon_1 D^{at}$, and requires exponentially small confidence intervals. This means we need $n$ to be exponentially big wrt the target $\epsilon$.

\begin{align}
\frac{C}{\sqrt{n}} &= \epsilon_1 D^{-at} \\
n &= \frac{C}{\epsilon_1^2} D^{2at} \\
\frac{C}{\epsilon_1^2} D^{2a(t+1)} - \frac{C}{\epsilon_1^2} D^{2at}
\end{align}

so to increase $t$ by one, you need exponentially more samples every time.

On mixing time, there is an additional thing to note since the policy of the algorithm is nonstationary, so that means the POMDP is not reaching a stationary distribution, but we still need to show that belief tracking gets close to the true belief.

\section{Other Approaches}

\begin{itemize}
	\item ROSS BAPOMDP
	\begin{itemize}
		\item
	\end{itemize}
	\item Finale
	\begin{itemize}
		\item Bayesian approach with particles
		\item uses QMDP approx at the leaves of stochastic forward search for planning
		\item can also use weighted\_stochastic, BOSS or BEB (but all perform similarly)
		\item iPOMDP and FFBS both perform similarly, so let's compare to FFBS since it's faster
		\item Finale graphs catch trials, which is not the online reward; we perform consistently better compared to the online reward on two-room and 2-sensor-tiger
		\item Domains:
	\end{itemize}
	\item FIB for tighter than QMDP upper bounds
	\item SARSOP, HVI, SOVI, PBVI and related for tighter lower bounds
	\item $AO^\star$ for online searching of the expanded belief tree, with lower/upper bounds at the leafs using one of the other approaches
	\item AEMS heuristic search
	\item POMCP with some Monte Carlo sampling to help in the tree search
	\item BAMCP for sampling BAMDP (not POMDPs?) - GRID5, GRID10, Double-loop, Dearden’s Maze domains
	\item RTDP-Bel and SOVI do learning?
	\item Policy search methods as well, like finite state controllers, or policy gradient methods
\end{itemize}

\begin{itemize}
	\item Tag Pineau et al. (2003) with 870 states
	\item RockSample Smith and Simmons (2004)
	\item HallWay
	\item Shuttle, 
\end{itemize}

What about Predictive State Representation (PSR)?

\end{document}
