\documentclass[10pt,english]{article}

\usepackage[utf8]{inputenc}
\usepackage[margin=1in,letterpaper]{geometry}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{amssymb}

\usepackage{graphicx}
\usepackage[parfill]{parskip}
%\usepackage{enumitem}

\title{Optimistic POMDP Notes}
\author{Daniel (Zhaohan) Guo}
\date{\today}

\begin{document}

\maketitle


\section{Difference in Belief States}

Suppose you have some POMDP $M$, and you have some policy $\pi$ for it. What's the difference in value between using the true belief, and using some very close estimated belief when the true belief is something else? Let $V_{b'}(b)$ mean that the policy starts off thinking the belief is $b'$ but the belief is actually $b$. Also suppose that $|b'-b|_{\infty}\leq \epsilon$
\begin{align}
V^{\pi}_{b'}(b) - V^{\pi}_b(b) &= b \cdot \alpha' - b\cdot \alpha \\
&= b \cdot \alpha' - b' \cdot \alpha' + b' \cdot \alpha' - b'\cdot \alpha + b' \cdot \alpha - b\cdot \alpha \\
&\geq (b-b') \cdot \alpha'+ (b'-b) \cdot \alpha \\
&\geq -\epsilon|S|V_{max} -\epsilon|S|V_{max} \\
&= -2\epsilon|S|V_{max}
\end{align}
so we know a couple of things
\begin{align}
b\cdot \alpha &\geq b \cdot \alpha' \\
b'\cdot \alpha' &\geq b' \cdot \alpha
\end{align}

\section{Approximate Belief Tracking}

Suppose $|O(\cdot|s,a) - \widehat{O}(\cdot|s,a)|_{1} \leq \epsilon_1$ and $|T(\cdot|s,a) - \widehat{T}(\cdot|s,a)|_{1} \leq \epsilon_2$ and you start out with belief $|b(\cdot) - \widehat{b}(\cdot)|_{1} = 0$.

We need some constraint that $\min_{|x|_1=1} \sum_{s} O(o|s,a)x(s)$ is bounded away from $0$ for any observation and action. Let's call it $\nu$.

Then before normalization (assuming $|B(\cdot) - \widehat{B}(\cdot)|_1 \leq \delta$)
\begin{align}
|B'(\cdot) - \widehat{B}'(\cdot)|_1 &= \sum_{s'}\left|O(o|s',a)\left[\sum_s T(s'|s,a)B(s)\right]-\widehat{O}(o|s',a)\left[\sum_s \widehat{T}(s'|s,a)\widehat{B}(s)\right]\right|\\
&\leq \sum_{s'}\left|O(o|s',a)\left[\sum_s T(s'|s,a)B(s)\right]-O(o|s',a)\left[\sum_s T(s'|s,a)\widehat{B}(s)\right]\right|\\
&+ \sum_{s'}\left|O(o|s',a)\left[\sum_s T(s'|s,a)\widehat{B}(s)\right]-O(o|s',a)\left[\sum_s \widehat{T}(s'|s,a)\widehat{B}(s)\right]\right|\\
&+ \sum_{s'}\left|O(o|s',a)\left[\sum_s \widehat{T}(s'|s,a)\widehat{B}(s)\right]-\widehat{O}(o|s',a)\left[\sum_s \widehat{T}(s'|s,a)\widehat{B}(s)\right]\right|\\
&\leq \sum_{s'}\sum_s T(s'|s,a)\left|B(s)-\widehat{B}(s)\right| \\
&+ \sum_{s'}\sum_s \widehat{B}(s)\left|T(s'|s,a)-\widehat{T}(s'|s,a)\right| \\
&+ \sum_{s'}\sum_s \widehat{T}(s'|s,a)\widehat{B}(s)\left|O(o|s',a)-\widehat{O}(o|s',a)\right| \\
&\leq \delta + \epsilon_2 + |S|\epsilon_1
\end{align}
so note that before normalization, it's just a linear error. So that after $t$ steps the error becomes
\begin{align}
|B_t(\cdot) - \widehat{B}_t(\cdot)|_1 \leq t\epsilon_2 + t|S|\epsilon_1
\end{align}
Let $A_i=\sum_s B_i(s)$ and $D_t = t\epsilon_2 + t|S|\epsilon_1$ for simplicity. Remember that we control how small $D_t$ is, so we can make it small enough that the rest of the inequalities hold. Note that $A_i\geq \nu$ by definition, which also means that $\nu > 0$ otherwise an impossible observation would've happened. Then after normalization:
\begin{align}
|b_t(\cdot)-\widehat{b}_t(\cdot)|_1 &\leq \sum_s \left| \frac{B_t(s)}{\prod_i A_i} - \frac{\widehat{B}_t(s)}{\prod_i \widehat{A}_i} \right| \\
&\leq \sum_s \left| \frac{B_t(s)}{\prod_i A_i} - \frac{B_t(s)+D_t}{\prod_i [A_i-D_t]} \right| \text{by making the right term as large as possible} \\
&\leq \sum_s \left| \frac{B_t(s)\prod_i A_i+D_t\prod_i A_i - B_t(s)\prod_i [A_i-D_t]}{\prod_i [A_i-D_t]\prod_i A_i} \right| \\
&\leq \sum_s \left| \frac{D_t\prod_t A_t +B_t(s)(\prod_t A_t-\prod_t [A_t-D_t])}{\prod_t [A_t-D_t]\prod_t A_t} \right| \\
&\leq \sum_s \left| \frac{D_t +2^tD_t}{\prod_t [A_t-D_t]\prod_t A_t} \right| \text{ if we expanded out that product and upper bounded $A_t\leq 1$ } \\
&\leq \sum_s \left| \frac{D_t +2^tD_t}{\prod_t [A_t-D_t]^2} \right|\\
&\leq |S|\frac{D_t +2^tD_t}{[\nu-D_t]^{2t}} \\
&\leq |S|\frac{D_t +2^tD_t}{\nu^{2t}-4^tD_t} \\
&\leq |S|\frac{1 +2^t}{(\nu^{2t}/D_t)-4^t}
\end{align}

Where we assume $D_t<1$ because its small, so that we can bound the expansions of $(1+D_t)^t\leq 1+2^tD_t$. Note that $\nu \leq A_t \leq 1$ as well.

Now a sufficient bound for $D_t$ is
\begin{align}
\frac{\nu^{2t}}{D_t} &\geq \frac{4^t\epsilon + |S|(1+2^t)}{\epsilon} \\
D_t &\leq \frac{\nu^{2t}\epsilon}{4^t\epsilon+|S|(1+2^t)} \\
&\leq O\left( \frac{\epsilon}{2|S|}\left(\frac{\nu}{2}\right)^{2t} \right)
\end{align}

so it's exponential in time in $\nu$ but linear with $\epsilon$.

\subsection{What happens when the minimum entry in the observation matrix is 0}

Note that unnormalized belief update is essentially the dot product of a column in the observation matrix with some vector that sums to 1 (combination of beliefs and transitions). Then you can imagine a case where you have $(z_1, z_2, 0)$ as your three entries of the observation matrix, and corresponding $(a_1,a_2,a_3)$ of the other vector. Then you also have the estimated versions $(\widehat{z}_1,\widehat{z}_2,\widehat{z}_3)$ and corresponding $(\widehat{a}_1,\widehat{a}_2,\widehat{a}_3)$. Now suppose $\widehat{z}_3=\epsilon$, so after dotting them, let's look at the third normalized entry. On one hand it will be $\frac{0}{z_1a_1+z_2a_2}=0$, and on the other hand it will be $\frac{\epsilon a_3}{\epsilon a_3+z_1a_1+z_2a_2}$ if we assume all the other estimates are completely correct. Now we can imagine letting $a_3\rightarrow 1$. Then that means $a_1$ and $a_2$ go to zero, so that other expression tends towards $1$, which is very different from what it should be which is $0$.

\section{Examples of Approximate Belief Tracking}

\subsection{Approximate Observations}

Suppose we have a 3 state POMDP $s_0, s_1, s_2$ where $s_0$ is the starting state. Let the policy be fixed, so that we can talk about the single transition and observation matrices. Suppose we know the transition probabilities with no error. $s_0$ transitions to $s_1$ and $s_2$ each with uniform probability. $s_1$ transitions to itself deterministically, and $s_2$ transitions to itself deterministically. There are three observations. Starting in $s_0$ gives observation $z_0$. $s_1$ gives observation $z_1$ with probability $p_1$ and $z_2$ with prob $1-p_1$, and $s_2$ gives $z_1$ with probability $p_2$ and $z_2$ with prob $1-p_2$. So the idea is that the system transitions randomly to either $s_1$ or $s_2$ and stays in that state, getting observations that give it a little bit of information about which state it is actually in, governed by the probs $p_1,p_2$.

Suppose $p_1 = 0.01$ and $p_2 = 0.02$. Suppose that we get a sequence of observations of just $z_1$. Then the belief will be updated to reflect that the more likely state is $s_2$. And this belief will continue to add more weight to $p_2$. However we may estimate those values to be off and instead think that $\hat{p}_1 = 0.02$ and $\hat{p}_2 = 0.01$. Now let's calculate just how much error there is for these two situations.

The initial belief is of course $\langle 0.5, 0.5 \rangle$. The transition matrix is just the identity for these two states, so after applying it it is still $\langle 0.5, 0.5 \rangle$. However now we come to the observations. And they end up scaling it by $\langle \frac{p_1 b_1}{p_1 b_1 + p_2 b_2} , \frac{p_2 b_2}{p_1 b_1 + p_2 b_2} \rangle$. Suppose WLG that $b_1 \leq b_2$ and $p_1 \leq p_2$, then $p_1 b_1 \leq p_2 b_2$, and
\begin{align}
\frac{p_2 b_2}{p_1 b_1 + p_2 b_2} / \frac{p_1 b_1}{p_1 b_1 + p_2 b_2} &= \frac{p_2 b_2}{p_1 b_1} \\
&= \left( \frac{p_2}{p_1} \right) \left( \frac{b_2}{b_1} \right)
\end{align}
so the ratio of the beliefs grows with a constant factor each iteration.

So if the estimates are off, then that means the constant factor is off.

\subsection{Quantifying additive and multiplicative differences}

Suppose $b_1 + b_2 = 1$ are non-negative reals and you know that $\frac{b_2}{b_1} = c$. Then what's $|b_2 - b_1|$? Well we can just solve the two equations
\begin{align}
& & b_1 + cb_1 &= 1 \\
\implies & & b_1 &= \frac{1}{1+c} \\
\implies & & b_2 &= \frac{c}{1+c} \\
\implies & & |b_2 - b_1 |  &= \frac{|1-c|}{1+c}
\end{align}

Note that $c$ and $\frac{1}{c}$ are symmetric cases.

Now suppose we have another belief vector with a different constant ratio. Then using inverse triangle inequality
\begin{align}
|b_1 - b_3| &= \left| \frac{1}{1+c} - \frac{1}{1+d}\right| \\
&= \frac{|d-c|}{(1+c)(1+d)} \\
&= \frac{|\delta|}{(1+c)(1+c+\delta)} \text{ assume $c \leq d$}\\
&\leq \frac{\delta}{(1+c)^2} \leq \epsilon\\
\implies \delta &\leq (1+c)^2 \epsilon \\
\implies \delta &\leq \epsilon
\end{align}

so we just need a small additive difference in the ratios. However that's the whole problem, since the ratios diverge multiplicatively.

\section{Generalized Induced Inequality}
Suppose $M$ and $M'$ are exactly the same on some set of known actions (so all states with that action is known). Suppose $\mathcal{A}$ is an algorithm that starts with policy $\pi$ and only changes it when an escape event occurs. Then let's consider all trajectories over the next $T$ steps. Those trajectories can be split up into two groups for each POMDP, ones where you escape and perform an unknown action, and ones where you don't and only perform known actions. For every trajectory where you don't escape, there is exactly the same trajectory for both POMDPs, because the dynamics are exactly the same, and since the policies are both the same (if we look at policies as mapping a history to an action) then they perform the same actions, and so they have identical trajectories. So there is this one-to-one mapping, that means the difference between trajectories of those kind is altogether $0$. It also means that the probability of escaping is exactly the same, since they have the exact same set of trajectories that don't escape (and everything adds up to $1$). Then it means the difference in the value functions is exactly determined by the trajectories that escape, which can be bounded.
\begin{align}
|V^{\mathcal{A}}_M(h,T) - V^{\pi}_{M'}(h,T)| &\leq [V^{\mathcal{A}}_M(h,T|\neg W)(1-w) - V^{\pi}_{M'}(h,T|\neg W)(1-w)] \\
& + [V^{\mathcal{A}}_M(h,T|W)w - V^{\pi}_{M'}(h,T|W)w]\\
&\leq V_{max}w
\end{align}


\section{Main Theorem with actions}

\subsection{Assumptions}

Let $\mathcal{A}$ be a POMDP algorithm such that it maintains a set $K_t$ of actions at every time step. Let $A_K$ denote the event that an action not in $K_t$ is taken in the next $T$ steps. $K_t$ can only have actions added to it, and only when $A_K$ occurs. Let $M$ denote the true underlying POMDP. Let $M_{K_t}$ denote the "optimistic" POMDP (equal to the true POMDP in parameters for actions in the known set) and for actions not in the known set, are set to something else. Then $\mathcal{A}$ maintains an approximate optimistic POMDP $\widehat{M}_{K_t}$ with its estimates and executes the optimal policy for it. $M_{K_t}$ can only change when $A_K$ occurs. Let $\widehat{\pi}_t$ be the policy that $\mathcal{A}$ has, which is the optimal policy for $\widehat{M}_{K_t}$. Then suppose the following hold for all time steps $t$ and histories $h_t$ with probability $1-\delta$:

\begin{enumerate}
\item $V^{*}_{M_{K_t}}(h_t) \geq V^{*}_M(h_t) - \epsilon/4$ (optimism)
\item $V^{\widehat{\pi}_t}_{M_{K_t}}(h_t) \geq V^{*}_{M_{K_t}}(h_t) -\epsilon/4$ (accuracy)
\item The total number of escape events from $K_t$ is bounded by $\zeta(\epsilon,\delta)$ (learning complexity)
\end{enumerate}

Then
\begin{align}
V^\mathcal{A}_{M}(h_t) &\geq V^{*}_{M}(h_t) - \epsilon
\end{align}
for all but
\begin{align}
O\left(\frac{\zeta(\epsilon,\delta)TV_{max}}{\epsilon}\log\left(\frac{1}{\delta}\right)\right)
\end{align}
time steps with probability $1-\delta$

\subsection{Proof}

Let $\widehat{\pi}_t$ be our algorithm's policy at time $t$ i.e. the optimal policy for $\widehat{M}_{K_t}$. Let $\pi_t$ be the optimal policy for $M_{K_t}$. Then from the generalized induced inequality:
\begin{align}
V^{\mathcal{A}}_M(h_t,T) &\geq V^{\widehat{\pi}}_{M_{K_t}}(h_t,T) - V_{max}\Pr(A_K)\\
&= V^{\widehat{\pi}}_{M_{K_t}}(b,T) - V_{max}\epsilon_1
\end{align}
Then we can get rid of the $T$ by the definition of $T$ by adding some error (or if we're doing periodic, we don't need to do this step)
\begin{align}
V^{\mathcal{A}}_M(h_t) &\geq V^{\widehat{\pi}}_M(h_t,T) - \epsilon/8\\
&= V^{\widehat{\pi}}_{M_{K_t}}(h_t,T) - V_{max}\epsilon_1 - \epsilon/8 \\
&\geq V^{\widehat{\pi}}_{M_{K_t}}(h_t) - V_{max}\epsilon_1 - \epsilon/4
\end{align}
and by the accuracy assumption (which we can get from simulation lemma paper and Theorem 2, for slightly different policies in the same POMDP, where one policy is optimal wrt a very slight variation of the other).
\begin{align}
V^{\mathcal{A}}_M(h_t) \geq V^{*}_{M_{K_t}}(h_t) - V_{max}\epsilon_1 - 2\epsilon/4
\end{align}
and from optimism
\begin{align}
V^{\mathcal{A}}_M(h_t) \geq V^{*}_{M}(h_t) - V_{max}\epsilon_1 - 3\epsilon/4
\end{align}

So to make sure all of that is $\leq \epsilon$, we should let $\epsilon_1 \leq \epsilon/(4V_{max})$. So that means by chernoff-hoeffding bounds, the number of times when the escape probability is greater than that is bounded with high probability by
\begin{align}
O\left(\frac{\zeta(\epsilon,\delta)TV_{max}}{\epsilon}\log\left(\frac{1}{\delta}\right)\right)
\end{align}

\section{Applying the Theorem}

\subsection{POMDP as MDP over histories in an Episodic Task}

Let's construct a MDP where states are histories i.e. $h=(a_1,o_1,a_2,o_2,\dots,a_t,o_t)$ of length $0\leq t \leq H$ where $H$ is the length of an episode. That means there are $(|A||O|)^H$ states. Then we can just apply a regular MDP algorithm like MBIE which will get us
\begin{align}
\tilde{O}\left( \frac{(|A||O|)^{2H}A\min(T,H)V_{max}}{\epsilon^3(1-\gamma)^4} \right)
\end{align}

The min comes from the fact that if the episode is short, then a high escape probability can potentially affect the whole episode. If we use a better MDP algorithm we can get a better dependence on $S$ and bring down the exponential to just $H$ rather than $2H$.

\subsection{Using MoM with Optimistic Planning}

Assume that the confidence intervals over the actions are $C/\sqrt{m}$ for each s--a pair where $C$ is some constant that may depend on the POMDP, and $m$ is the number of visits. Assume that we have some sort of belief tracking lemma that says that if the POMDP parameters are $D(\epsilon)$ close then the belief is $\epsilon$ away from the true belief in L1 norm (probably exponential in $\epsilon$ and some other parameters).

So at every good time step, MoM estimates the parameters, and gets confidence intervals, and the algorithm constructs an optimistic POMDP $M_{K_t}$, and approximates it with $\widehat{M}_{K_t}$. By the definition of optimism, $M_{K_t}$ satisfies the optimism condition. Now onto the accuracy condition. If an escape event does not occur, then belief tracking will be close. That means that
\begin{align}
V^{\widehat{\pi}}_{\widehat{M}_{K_t,b'(t)}}(b_t) \geq V^{\widehat{\pi}}_{\widehat{M}_{K_t}}(b_t) - 2\epsilon_1|S|V_{max}
\end{align}
if the POMDP parameters are $D(\epsilon_1)$ close. Next we apply theorem 3 from the simulation lemma paper, so get that
\begin{align}
V^{\widehat{\pi}}_{\widehat{M}_{K_t}}(b_t) \geq V^{\widehat{\pi}}_{M_{K_t}}(b_t) - \frac{\epsilon_r}{1-\gamma} + \frac{\gamma R_{max}}{(1-\gamma)^2}(\epsilon_t|S| + \epsilon_o |O|)
\end{align}
where $\epsilon_r,\epsilon_t,\epsilon_o$ are how close the reward, transition, and observation matrices are. Combining these two together gets us
\begin{align}
V^{\widehat{\pi}}_{\widehat{M}_{K_t}}(h_t) &= V^{\widehat{\pi}}_{\widehat{M}_{K_t}}(b'_t) \\
 &\geq V^{\widehat{\pi}}_{\widehat{M}_{K_t,b'(t)}}(b_t) \\
 &\geq V^{\widehat{\pi}}_{\widehat{M}_{K_t}}(b_t) - 2\epsilon_1|S|V_{max} \\
  &\geq V^{\widehat{\pi}}_{M_{K_t}}(b_t) - \frac{\epsilon_r}{1-\gamma} + \frac{\gamma R_{max}}{(1-\gamma)^2}(\epsilon_t|S| + \epsilon_o |O|) - 2\epsilon_1|S|V_{max}
\end{align}
where $b'_t$ is the belief that's tracked by $\widehat{M}_{K_t}$ and $b_t$ is the belief that's tracked by $M_{K_t}$. Now if we pick appropriate $\epsilon$'s, we can use this to satisfy the accuracy condition. A sufficient choice for all those intermediate epsilons is something like $\epsilon' = \min(D(\epsilon/(8|S|V_{max})),\epsilon/(8(|S|+|O|)R_{max}(1-\gamma)^2))$ to get all that error to be less than $\epsilon/4$. Finally we have the learning complexity. We need all the parameters in the known set to be $\epsilon'$ accurate, and it's really the $D(\cdot)$ that dominates because that's exponential in the length of the episode. So simply we need the confidence intervals to shrink to $\epsilon'/|S|$. Then
\begin{align}
m &\geq \frac{|S|^2}{\epsilon'^2 C} \\
&=\frac{|S|^2}{D(\epsilon/(8|S|V_{max})^2 C}
\end{align}
and so the learning complexity is
\begin{align}
\tilde{O}\left( \frac{|S|^3|A|}{D(\epsilon/(8|S|V_{max})^2 C} \right)
\end{align}
so applying the theorem means the sample complexity is
\begin{align}
O\left(\frac{|S|^3|A|TV_{max}}{\epsilon D(\epsilon/(8|S|V_{max})^2 C}\log\left(\frac{1}{\delta}\right)\right)
\end{align}
but also note that if there is an escape event, then MoM must spend the next episode's beginning trying out an unknown action, which means it wastes a whole next episode. So we need to add it into it. And sub in the error in belief tracking.
\begin{align}
O\left(\frac{|S|^7|A|(T+H)V_{max}^3 \left(\frac{2}{\nu}\right)^{4H}}{\epsilon^3 C}\log\left(\frac{1}{\delta}\right)\right)
\end{align}
\end{document}
